Hyperparameter Tuning Analysis
==================================================

1. Bagging Hyperparameters
------------------------------

Effect of n_estimators (number of trees):
- Increasing the number of trees generally improves performance up to a point
- Diminishing returns are observed beyond a certain number
- More trees increase computational cost linearly

Effect of max_samples (bootstrap sample size):
- Smaller samples create more diverse trees but may miss important patterns
- Larger samples capture more information but may reduce diversity
- The optimal value depends on the dataset characteristics

Effect of max_depth (tree depth):
- Deeper trees can model more complex relationships but risk overfitting
- Shallower trees generalize better but may underfit
- Ensemble methods can use deeper trees than individual models due to variance reduction

Effect of criterion (splitting criterion):
- Gini vs. Entropy often show similar performance
- Gini is computationally more efficient
- Entropy might perform better in some specific datasets

2. AdaBoost Hyperparameters
------------------------------

Effect of n_estimators (number of weak learners):
- More weak learners can reduce both bias and variance
- Too many weak learners can lead to overfitting
- The optimal number depends on the learning rate and data complexity

Effect of learning_rate (shrinkage):
- Lower learning rates require more estimators but often generalize better
- Higher learning rates learn faster but may overfit
- Typical values range from 0.01 to 1.0

Effect of max_depth (weak learner complexity):
- Decision stumps (depth=1) are traditional for AdaBoost
- Deeper trees can improve performance but may need lower learning rates
- Increasing depth makes the model more prone to overfitting

Effect of criterion (splitting criterion):
- Similar effects as in Bagging
- The choice between Gini and Entropy is less significant compared to other parameters

3. Optimal Configurations
------------------------------

Based on the experiments, the best configurations are:

Best Bagging Configuration (Bagging_config_4):
- n_estimators: 15
- max_samples: 0.8
- max_depth: 5
- criterion: entropy
- model_name: Bagging_config_4
Performance: {'accuracy': 0.8188888888888889, 'precision': 0.6757281553398058, 'recall': 0.3493975903614458, 'f1_score': 0.4606221045665123, 'roc_auc': 0.6977474681832353}

Best AdaBoost Configuration (AdaBoost_config_4):
- n_estimators: 50
- learning_rate: 0.7
- max_depth: 2
- criterion: gini
- model_name: AdaBoost_config_4
Performance: {'accuracy': 0.8171111111111111, 'precision': 0.6706114398422091, 'recall': 0.3413654618473896, 'f1_score': 0.45242847638057215, 'roc_auc': 0.7751482528286662}

4. Conclusion
------------------------------

The hyperparameter tuning experiments demonstrate the importance of properly configuring
ensemble methods. The optimal parameters depend on the dataset characteristics and the
balance between bias and variance that needs to be achieved.

For both Bagging and AdaBoost, increasing the number of estimators and finding the right
tree depth are crucial factors. AdaBoost additionally benefits from proper learning rate tuning.

Overall, the tuned ensemble models show significant improvements over the base models,
highlighting the value of hyperparameter optimization in machine learning practice.