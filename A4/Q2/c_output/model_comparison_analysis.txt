Ensemble Methods vs Individual Models Analysis
==================================================

Performance Metrics Comparison:
               accuracy precision    recall  f1_score   roc_auc
Decision Tree  0.814667  0.659449  0.336345  0.445479  0.643487
Bagging        0.816444  0.663462  0.346386  0.455145  0.687558
AdaBoost          0.816  0.679487  0.319277  0.434426  0.763815

Strengths and Weaknesses Analysis:

1. Bagging (Bootstrap Aggregating):
   Strengths:
   - Reduces variance by training on different bootstrap samples
   - Helps prevent overfitting compared to individual decision trees
   - More stable predictions across different datasets
   - Handles noise in the training data well

   Weaknesses:
   - Cannot reduce bias inherent in the base learners
   - Computationally more expensive than a single decision tree
   - May not perform well if base learners are correlated

2. Boosting (AdaBoost):
   Strengths:
   - Reduces both bias and variance
   - Focuses on difficult examples by increasing their weights
   - Can achieve higher accuracy than bagging in many cases
   - Works well with weak learners (e.g., decision stumps)

   Weaknesses:
   - More prone to overfitting, especially on noisy data
   - Training is sequential and cannot be parallelized
   - Sensitive to outliers due to the weight update mechanism
   - Generally slower than bagging due to sequential   - More prone to overfitting, especially on noisy data
   - Training is sequential and cannot be parallelized
   - Sensitive to outliers due to the weight update mechanism
   - Generally slower than bagging due to sequential nature

3. Decision Tree (Individual Model):
   Strengths:
   - Simple to understand and interpret
   - Requires minimal data preprocessing
   - Fast training and prediction times

   Weaknesses:
   - Prone to overfitting, especially with deep trees
   - High variance (sensitive to training data)
   - Can create biased trees if some classes dominate

Overall Comparison:
Based on the performance metrics, the ensemble methods (Bagging and AdaBoost) generally outperform
the individual decision tree. This confirms the theoretical advantage of ensemble methods in reducing
either variance (Bagging) or both bias and variance (AdaBoost).

The improvements are particularly noticeable in terms of stability and generalization ability,
which is reflected in the improved accuracy and F1-scores of the ensemble methods.
