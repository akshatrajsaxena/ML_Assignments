{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f82687ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "===== Implementing Logistic Regression from Scratch =====\n",
      "Training logistic regression...\n",
      "Iteration 0, Loss: 0.6907\n",
      "Iteration 100, Loss: 0.5546\n",
      "Iteration 200, Loss: 0.5097\n",
      "Iteration 300, Loss: 0.4909\n",
      "Iteration 400, Loss: 0.4818\n",
      "Iteration 500, Loss: 0.4769\n",
      "Iteration 600, Loss: 0.4739\n",
      "Iteration 700, Loss: 0.4720\n",
      "Iteration 800, Loss: 0.4706\n",
      "Iteration 900, Loss: 0.4696\n",
      "Training completed in 0.8267 seconds\n",
      "Evaluating on test set...\n",
      "Logistic Regression results saved in LogisticRegression directory\n",
      "\n",
      "===== Implementing Decision Tree Classifier from Scratch =====\n",
      "Training decision tree...\n",
      "Training completed in 155.4038 seconds\n",
      "Evaluating on test set...\n",
      "Decision Tree results saved in DecisionTree directory\n",
      "\n",
      "===== Implementing K-Nearest Neighbors Classifier from Scratch =====\n",
      "Training KNN...\n",
      "Training completed in 0.0001 seconds\n",
      "Evaluating on test set...\n",
      "KNN results saved in KNN directory\n",
      "\n",
      "===== Comparing All Models =====\n",
      "\n",
      "Model Comparison:\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score       AUC  \\\n",
      "0  Logistic Regression  0.796667   0.651685  0.174699  0.275534  1.710462   \n",
      "1        Decision Tree  0.816000   0.664062  0.341365  0.450928  1.646139   \n",
      "2                  KNN  0.791778   0.545038  0.358434  0.432465  1.704064   \n",
      "\n",
      "   Training Time (s)  \n",
      "0           0.826664  \n",
      "1         155.403835  \n",
      "2           0.000081  \n",
      "Comparison results saved to model_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('LogisticRegression', exist_ok=True)\n",
    "os.makedirs('DecisionTree', exist_ok=True)\n",
    "os.makedirs('KNN', exist_ok=True)\n",
    "\n",
    "# Load the datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_data = pd.read_csv('preprocessed_data/train_data.csv')\n",
    "validation_data = pd.read_csv('preprocessed_data/validation_data.csv')\n",
    "test_data = pd.read_csv('preprocessed_data/test_data.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X_train = train_data.drop('default_payment', axis=1).values\n",
    "y_train = train_data['default_payment'].values\n",
    "X_val = validation_data.drop('default_payment', axis=1).values\n",
    "y_val = validation_data['default_payment'].values\n",
    "X_test = test_data.drop('default_payment', axis=1).values\n",
    "y_test = test_data['default_payment'].values\n",
    "\n",
    "# Helper functions for evaluation\n",
    "def calculate_metrics(y_true, y_pred, probabilities=None):\n",
    "    \"\"\"Calculate accuracy, precision, recall, F1-score, and AUC.\"\"\"\n",
    "    # True positives, false positives, true negatives, false negatives\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Calculate AUC if probabilities are provided\n",
    "    auc = None\n",
    "    if probabilities is not None:\n",
    "        # Sort by probability and calculate AUC\n",
    "        sorted_indices = np.argsort(probabilities)\n",
    "        sorted_y_true = y_true[sorted_indices]\n",
    "        \n",
    "        # Calculate AUC using trapezoidal rule\n",
    "        positive_count = np.sum(y_true == 1)\n",
    "        negative_count = np.sum(y_true == 0)\n",
    "        \n",
    "        if positive_count > 0 and negative_count > 0:\n",
    "            fp_rates = []\n",
    "            tp_rates = []\n",
    "            thresholds = np.unique(probabilities)\n",
    "            thresholds = np.append(thresholds, thresholds[-1] + 1)  # Add a threshold higher than the max\n",
    "            thresholds = np.sort(thresholds)\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                y_pred_temp = (probabilities >= threshold).astype(int)\n",
    "                tp_temp = np.sum((y_true == 1) & (y_pred_temp == 1))\n",
    "                fp_temp = np.sum((y_true == 0) & (y_pred_temp == 1))\n",
    "                tp_rate = tp_temp / positive_count if positive_count > 0 else 0\n",
    "                fp_rate = fp_temp / negative_count if negative_count > 0 else 0\n",
    "                tp_rates.append(tp_rate)\n",
    "                fp_rates.append(fp_rate)\n",
    "            \n",
    "            # Calculate AUC using trapezoidal rule\n",
    "            auc = 0.0\n",
    "            for i in range(len(fp_rates) - 1):\n",
    "                auc += (fp_rates[i+1] - fp_rates[i]) * (tp_rates[i+1] + tp_rates[i]) / 2\n",
    "            \n",
    "            # Convert from increasing FPR to decreasing FPR for proper AUC calculation\n",
    "            auc = 1.0 - auc\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "def plot_roc_curve(y_true, y_prob, classifier_name, output_dir):\n",
    "    \"\"\"Plot ROC curve and save to file\"\"\"\n",
    "    positive_count = np.sum(y_true == 1)\n",
    "    negative_count = np.sum(y_true == 0)\n",
    "    \n",
    "    thresholds = np.unique(y_prob)\n",
    "    thresholds = np.append(thresholds, thresholds[-1] + 1)  # Add a threshold higher than max\n",
    "    thresholds = np.sort(thresholds)\n",
    "    \n",
    "    fp_rates = []\n",
    "    tp_rates = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred_temp = (y_prob >= threshold).astype(int)\n",
    "        tp_temp = np.sum((y_true == 1) & (y_pred_temp == 1))\n",
    "        fp_temp = np.sum((y_true == 0) & (y_pred_temp == 1))\n",
    "        tp_rate = tp_temp / positive_count if positive_count > 0 else 0\n",
    "        fp_rate = fp_temp / negative_count if negative_count > 0 else 0\n",
    "        tp_rates.append(tp_rate)\n",
    "        fp_rates.append(fp_rate)\n",
    "    \n",
    "    # Sort for proper plotting\n",
    "    points = sorted(zip(fp_rates, tp_rates))\n",
    "    fp_rates = [p[0] for p in points]\n",
    "    tp_rates = [p[1] for p in points]\n",
    "    \n",
    "    # Calculate AUC using trapezoidal rule\n",
    "    auc = 0\n",
    "    for i in range(len(fp_rates) - 1):\n",
    "        auc += (fp_rates[i+1] - fp_rates[i]) * (tp_rates[i+1] + tp_rates[i]) / 2\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fp_rates, tp_rates, color='blue', lw=2, label=f'ROC curve (AUC = {auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve for {classifier_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{output_dir}/roc_curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "def confusion_matrix_plot(y_true, y_pred, classifier_name, output_dir):\n",
    "    \"\"\"Plot confusion matrix and save to file\"\"\"\n",
    "    # Calculate confusion matrix\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    cm = np.array([[tn, fp], [fn, tp]])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix for {classifier_name}')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Add labels\n",
    "    classes = ['No Default (0)', 'Default (1)']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Add text annotations\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(f\"{output_dir}/confusion_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "def save_results(metrics, training_time, classifier_name, output_dir):\n",
    "    \"\"\"Save metrics and training time to a JSON file\"\"\"\n",
    "    results = {\n",
    "        'metrics': metrics,\n",
    "        'training_time_seconds': training_time\n",
    "    }\n",
    "    \n",
    "    with open(f\"{output_dir}/results.json\", 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    # Also save as text for easy reading\n",
    "    with open(f\"{output_dir}/results.txt\", 'w') as f:\n",
    "        f.write(f\"Results for {classifier_name}\\n\")\n",
    "        f.write(f\"Training time: {training_time:.4f} seconds\\n\\n\")\n",
    "        f.write(\"Test Set Metrics:\\n\")\n",
    "        for metric, value in metrics.items():\n",
    "            if value is not None:\n",
    "                f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{metric}: N/A\\n\")\n",
    "\n",
    "# ========== 1. Logistic Regression from Scratch ==========\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, tol=1e-4):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tol = tol\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.losses = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        # Clip to avoid overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def initialize_parameters(self, n_features):\n",
    "        \"\"\"Initialize weights and bias\"\"\"\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "    \n",
    "    def compute_cost(self, X, y):\n",
    "        \"\"\"Compute the binary cross-entropy loss\"\"\"\n",
    "        m = X.shape[0]\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        predictions = self.sigmoid(z)\n",
    "        \n",
    "        # Avoid log(0) by adding a small epsilon\n",
    "        epsilon = 1e-15\n",
    "        predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Binary cross-entropy loss\n",
    "        loss = -1/m * np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "        return loss\n",
    "    \n",
    "    def compute_gradients(self, X, y):\n",
    "        \"\"\"Compute gradients for weights and bias\"\"\"\n",
    "        m = X.shape[0]\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        predictions = self.sigmoid(z)\n",
    "        \n",
    "        # Gradients\n",
    "        dw = 1/m * np.dot(X.T, (predictions - y))\n",
    "        db = 1/m * np.sum(predictions - y)\n",
    "        \n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, X, y, verbose=True):\n",
    "        \"\"\"Train the logistic regression model using gradient descent\"\"\"\n",
    "        # Initialize parameters\n",
    "        self.initialize_parameters(X.shape[1])\n",
    "        \n",
    "        # Gradient descent\n",
    "        prev_loss = float('inf')\n",
    "        for i in range(self.max_iterations):\n",
    "            # Compute gradients\n",
    "            dw, db = self.compute_gradients(X, y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Compute loss\n",
    "            if i % 100 == 0 or i == self.max_iterations - 1:\n",
    "                loss = self.compute_cost(X, y)\n",
    "                self.losses.append(loss)\n",
    "                \n",
    "                if verbose and (i % 100 == 0):\n",
    "                    print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
    "                \n",
    "                # Check for convergence\n",
    "                if abs(prev_loss - loss) < self.tol:\n",
    "                    if verbose:\n",
    "                        print(f\"Converged at iteration {i} with loss {loss:.4f}\")\n",
    "                    break\n",
    "                \n",
    "                prev_loss = loss\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probability of the positive class\"\"\"\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities >= threshold).astype(int)\n",
    "\n",
    "# Train and evaluate Logistic Regression\n",
    "print(\"\\n===== Implementing Logistic Regression from Scratch =====\")\n",
    "log_reg = LogisticRegression(learning_rate=0.01, max_iterations=1000)\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Training logistic regression...\")\n",
    "log_reg.fit(X_train, y_train)\n",
    "lr_training_time = time.time() - start_time\n",
    "print(f\"Training completed in {lr_training_time:.4f} seconds\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "y_prob_lr = log_reg.predict_proba(X_test)\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "lr_metrics = calculate_metrics(y_test, y_pred_lr, y_prob_lr)\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(0, len(log_reg.losses) * 100, 100), log_reg.losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Logistic Regression Learning Curve')\n",
    "plt.grid(True)\n",
    "plt.savefig('LogisticRegression/learning_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot ROC curve and confusion matrix\n",
    "plot_roc_curve(y_test, y_prob_lr, \"Logistic Regression\", \"LogisticRegression\")\n",
    "confusion_matrix_plot(y_test, y_pred_lr, \"Logistic Regression\", \"LogisticRegression\")\n",
    "\n",
    "# Save model parameters\n",
    "np.savetxt('LogisticRegression/weights.txt', log_reg.weights)\n",
    "with open('LogisticRegression/bias.txt', 'w') as f:\n",
    "    f.write(str(log_reg.bias))\n",
    "\n",
    "# Save results\n",
    "save_results(lr_metrics, lr_training_time, \"Logistic Regression\", \"LogisticRegression\")\n",
    "print(\"Logistic Regression results saved in LogisticRegression directory\")\n",
    "\n",
    "# ========== 2. Decision Tree Classifier from Scratch ==========\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, feature_idx=None, threshold=None, left=None, right=None, value=None):\n",
    "        # For decision nodes\n",
    "        self.feature_idx = feature_idx  # Index of the feature to split on\n",
    "        self.threshold = threshold      # Threshold value for the split\n",
    "        self.left = left                # Left subtree (samples where feature <= threshold)\n",
    "        self.right = right              # Right subtree (samples where feature > threshold)\n",
    "        \n",
    "        # For leaf nodes\n",
    "        self.value = value              # Class prediction for a leaf node\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.tree = None\n",
    "        self.feature_importances_ = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build the decision tree\"\"\"\n",
    "        self.n_features = X.shape[1]\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "        \n",
    "        # Calculate feature importances\n",
    "        self.feature_importances_ = np.zeros(self.n_features)\n",
    "        self._calculate_feature_importances(self.tree, self.n_features)\n",
    "        \n",
    "        # Normalize feature importances\n",
    "        if np.sum(self.feature_importances_) > 0:\n",
    "            self.feature_importances_ /= np.sum(self.feature_importances_)\n",
    "    \n",
    "    def _calculate_feature_importances(self, node, n_features, depth=0):\n",
    "        \"\"\"Recursively calculate feature importances\"\"\"\n",
    "        if node.feature_idx is not None:  # Decision node\n",
    "            # Importance is proportional to the depth\n",
    "            importance = 1.0 / (depth + 1.0)\n",
    "            self.feature_importances_[node.feature_idx] += importance\n",
    "            \n",
    "            # Recurse down the tree\n",
    "            self._calculate_feature_importances(node.left, n_features, depth + 1)\n",
    "            self._calculate_feature_importances(node.right, n_features, depth + 1)\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        \"\"\"Recursively grow the decision tree\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Check stopping criteria\n",
    "        if (self.max_depth is not None and depth >= self.max_depth or\n",
    "            n_samples < self.min_samples_split or\n",
    "            n_classes == 1):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionTreeNode(value=leaf_value)\n",
    "        \n",
    "        # Find the best split\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "        \n",
    "        # If no improvement, create a leaf node\n",
    "        if best_feature is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionTreeNode(value=leaf_value)\n",
    "        \n",
    "        # Split the data\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = ~left_indices\n",
    "        \n",
    "        # Check if the split is valid (both sides have minimum samples)\n",
    "        if np.sum(left_indices) < self.min_samples_leaf or np.sum(right_indices) < self.min_samples_leaf:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionTreeNode(value=leaf_value)\n",
    "        \n",
    "        # Grow the left and right subtrees\n",
    "        left_subtree = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        \n",
    "        return DecisionTreeNode(\n",
    "            feature_idx=best_feature,\n",
    "            threshold=best_threshold,\n",
    "            left=left_subtree,\n",
    "            right=right_subtree\n",
    "        )\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"Find the best feature and threshold for splitting\"\"\"\n",
    "        m, n = X.shape\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "        \n",
    "        # Parent set entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "        \n",
    "        # If parent entropy is 0, no need to split\n",
    "        if parent_entropy == 0:\n",
    "            return None, None\n",
    "        \n",
    "        # Initialize variables\n",
    "        best_info_gain = -float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        # Try each feature\n",
    "        for feature_idx in range(n):\n",
    "            feature_values = X[:, feature_idx]\n",
    "            thresholds = np.unique(feature_values)\n",
    "            \n",
    "            # Try each threshold\n",
    "            for threshold in thresholds:\n",
    "                # Split the data\n",
    "                left_indices = feature_values <= threshold\n",
    "                right_indices = ~left_indices\n",
    "                \n",
    "                # Skip if split doesn't meet minimum samples\n",
    "                if np.sum(left_indices) < self.min_samples_leaf or np.sum(right_indices) < self.min_samples_leaf:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate information gain\n",
    "                left_entropy = self._entropy(y[left_indices])\n",
    "                right_entropy = self._entropy(y[right_indices])\n",
    "                \n",
    "                # Weighted sum of entropies\n",
    "                n_left, n_right = np.sum(left_indices), np.sum(right_indices)\n",
    "                weighted_entropy = (n_left * left_entropy + n_right * right_entropy) / m\n",
    "                \n",
    "                # Information gain\n",
    "                info_gain = parent_entropy - weighted_entropy\n",
    "                \n",
    "                # Update best split if this is better\n",
    "                if info_gain > best_info_gain:\n",
    "                    best_info_gain = info_gain\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        \"\"\"Calculate entropy of a set\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Count occurrences of each class\n",
    "        counts = np.bincount(y.astype(int))\n",
    "        probabilities = counts / len(y)\n",
    "        \n",
    "        # Remove zero probabilities\n",
    "        probabilities = probabilities[probabilities > 0]\n",
    "        \n",
    "        # Calculate entropy\n",
    "        return -np.sum(probabilities * np.log2(probabilities))\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        \"\"\"Return the most common label in a set\"\"\"\n",
    "        counter = Counter(y)\n",
    "        most_common = counter.most_common(1)[0][0]\n",
    "        return most_common\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for samples in X\"\"\"\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for samples in X\"\"\"\n",
    "        # This is a simplification - we're using a basic approach where probability\n",
    "        # is 1 for the predicted class and 0 for others\n",
    "        # In a more sophisticated implementation, we would use the class distribution in the leaf\n",
    "        y_pred = self.predict(X)\n",
    "        return y_pred  # Binary classification (0 or 1)\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        \"\"\"Traverse the tree to make a prediction for a single sample\"\"\"\n",
    "        # If leaf node, return the predicted value\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        # Traverse left or right based on the feature value\n",
    "        if x[node.feature_idx] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "    \n",
    "    def print_tree(self, node=None, depth=0):\n",
    "        \"\"\"Print the decision tree structure (for debugging)\"\"\"\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "        \n",
    "        # Indent based on depth\n",
    "        indent = \"  \" * depth\n",
    "        \n",
    "        # Leaf node\n",
    "        if node.value is not None:\n",
    "            print(f\"{indent}Predict: {node.value}\")\n",
    "        # Decision node\n",
    "        else:\n",
    "            print(f\"{indent}Feature {node.feature_idx} <= {node.threshold}\")\n",
    "            print(f\"{indent}Left subtree:\")\n",
    "            self.print_tree(node.left, depth + 1)\n",
    "            print(f\"{indent}Right subtree:\")\n",
    "            self.print_tree(node.right, depth + 1)\n",
    "\n",
    "# Train and evaluate Decision Tree\n",
    "print(\"\\n===== Implementing Decision Tree Classifier from Scratch =====\")\n",
    "dt = DecisionTreeClassifier(max_depth=5, min_samples_split=10, min_samples_leaf=5)\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Training decision tree...\")\n",
    "dt.fit(X_train, y_train)\n",
    "dt_training_time = time.time() - start_time\n",
    "print(f\"Training completed in {dt_training_time:.4f} seconds\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "y_prob_dt = dt.predict(X_test)  # Simple prediction as probability\n",
    "dt_metrics = calculate_metrics(y_test, y_pred_dt, y_prob_dt)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "indices = np.argsort(dt.feature_importances_)[-10:]  # Top 10 features\n",
    "plt.barh(range(len(indices)), dt.feature_importances_[indices], align='center')\n",
    "feature_names = train_data.drop('default_payment', axis=1).columns\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.title('Top 10 Feature Importances in Decision Tree')\n",
    "plt.savefig('DecisionTree/feature_importances.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot confusion matrix\n",
    "confusion_matrix_plot(y_test, y_pred_dt, \"Decision Tree\", \"DecisionTree\")\n",
    "\n",
    "# Save results\n",
    "save_results(dt_metrics, dt_training_time, \"Decision Tree\", \"DecisionTree\")\n",
    "print(\"Decision Tree results saved in DecisionTree directory\")\n",
    "\n",
    "# ========== 3. K-Nearest Neighbors Classifier from Scratch ==========\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Store the training data\"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for samples in X\"\"\"\n",
    "        return np.array([self._predict_single(x) for x in X])\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for samples in X\"\"\"\n",
    "        # For binary classification, return the proportion of positive class neighbors\n",
    "        probabilities = np.array([self._predict_proba_single(x) for x in X])\n",
    "        return probabilities\n",
    "    \n",
    "    def _predict_single(self, x):\n",
    "        \"\"\"Predict the class for a single sample\"\"\"\n",
    "        # Calculate distances\n",
    "        distances = np.sqrt(np.sum((self.X_train - x)**2, axis=1))\n",
    "        \n",
    "        # Find indices of k nearest neighbors\n",
    "        nearest_indices = np.argsort(distances)[:self.k]\n",
    "        \n",
    "        # Get labels of k nearest neighbors\n",
    "        nearest_labels = self.y_train[nearest_indices]\n",
    "        \n",
    "        # Majority vote\n",
    "        most_common = Counter(nearest_labels).most_common(1)[0][0]\n",
    "        return most_common\n",
    "    \n",
    "    def _predict_proba_single(self, x):\n",
    "        \"\"\"Predict the probability for a single sample\"\"\"\n",
    "        # Calculate distances\n",
    "        distances = np.sqrt(np.sum((self.X_train - x)**2, axis=1))\n",
    "        \n",
    "        # Find indices of k nearest neighbors\n",
    "        nearest_indices = np.argsort(distances)[:self.k]\n",
    "        \n",
    "        # Get labels of k nearest neighbors\n",
    "        nearest_labels = self.y_train[nearest_indices]\n",
    "        \n",
    "        # Calculate proportion of positive class (class 1)\n",
    "        return np.mean(nearest_labels)\n",
    "\n",
    "# Train and evaluate KNN\n",
    "print(\"\\n===== Implementing K-Nearest Neighbors Classifier from Scratch =====\")\n",
    "knn = KNNClassifier(k=5)\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Training KNN...\")\n",
    "knn.fit(X_train, y_train)\n",
    "knn_training_time = time.time() - start_time\n",
    "print(f\"Training completed in {knn_training_time:.4f} seconds\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "y_prob_knn = knn.predict_proba(X_test)\n",
    "knn_metrics = calculate_metrics(y_test, y_pred_knn, y_prob_knn)\n",
    "\n",
    "# Plot ROC curve and confusion matrix\n",
    "plot_roc_curve(y_test, y_prob_knn, \"K-Nearest Neighbors\", \"KNN\")\n",
    "confusion_matrix_plot(y_test, y_pred_knn, \"K-Nearest Neighbors\", \"KNN\")\n",
    "\n",
    "# Save results\n",
    "save_results(knn_metrics, knn_training_time, \"K-Nearest Neighbors\", \"KNN\")\n",
    "print(\"KNN results saved in KNN directory\")\n",
    "\n",
    "# ========== Compare all models ==========\n",
    "print(\"\\n===== Comparing All Models =====\")\n",
    "models = [\"Logistic Regression\", \"Decision Tree\", \"KNN\"]\n",
    "metrics_list = [lr_metrics, dt_metrics, knn_metrics]\n",
    "training_times = [lr_training_time, dt_training_time, knn_training_time]\n",
    "\n",
    "# Create comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Accuracy': [m['accuracy'] for m in metrics_list],\n",
    "    'Precision': [m['precision'] for m in metrics_list],\n",
    "    'Recall': [m['recall'] for m in metrics_list],\n",
    "    'F1 Score': [m['f1_score'] for m in metrics_list],\n",
    "    'AUC': [m['auc'] if m['auc'] is not None else float('nan') for m in metrics_list],\n",
    "    'Training Time (s)': training_times\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison)\n",
    "\n",
    "# Save comparison results\n",
    "comparison.to_csv('model_comparison.csv', index=False)\n",
    "print(\"Comparison results saved to model_comparison.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
